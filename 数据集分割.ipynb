{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b06be26b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始数据集划分\n",
      "*********************************1*************************************\n",
      "1类按照0.75：0：0.25的比例划分完成，一共20张图片\n",
      "训练集D:/ramanfig/new\\train\\1：16张\n",
      "验证集D:/ramanfig/new\\val\\1：0张\n",
      "测试集D:/ramanfig/new\\test\\1：4张\n",
      "*********************************2*************************************\n",
      "2类按照0.75：0：0.25的比例划分完成，一共20张图片\n",
      "训练集D:/ramanfig/new\\train\\2：16张\n",
      "验证集D:/ramanfig/new\\val\\2：0张\n",
      "测试集D:/ramanfig/new\\test\\2：4张\n",
      "*********************************3*************************************\n",
      "3类按照0.75：0：0.25的比例划分完成，一共20张图片\n",
      "训练集D:/ramanfig/new\\train\\3：16张\n",
      "验证集D:/ramanfig/new\\val\\3：0张\n",
      "测试集D:/ramanfig/new\\test\\3：4张\n",
      "*********************************4*************************************\n",
      "4类按照0.75：0：0.25的比例划分完成，一共20张图片\n",
      "训练集D:/ramanfig/new\\train\\4：16张\n",
      "验证集D:/ramanfig/new\\val\\4：0张\n",
      "测试集D:/ramanfig/new\\test\\4：4张\n"
     ]
    },
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: 'D:/ramanfig\\\\new\\\\test'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-68a65b3e9b1c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     85\u001b[0m     \u001b[0msrc_data_folder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"D:/ramanfig\"\u001b[0m   \u001b[1;31m# todo 修改你的原始数据集路径\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m     \u001b[0mtarget_data_folder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"D:/ramanfig/new\"\u001b[0m  \u001b[1;31m# todo 修改为你要存放的路径\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 87\u001b[1;33m     \u001b[0mdata_set_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc_data_folder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_data_folder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-9-68a65b3e9b1c>\u001b[0m in \u001b[0;36mdata_set_split\u001b[1;34m(src_data_folder, target_data_folder, train_scale, val_scale, test_scale)\u001b[0m\n\u001b[0;32m     60\u001b[0m             \u001b[0msrc_img_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcurrent_class_data_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcurrent_all_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mcurrent_idx\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[0mtrain_stop_flag\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 62\u001b[1;33m                 \u001b[0mcopy2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc_img_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_folder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     63\u001b[0m                 \u001b[1;31m# print(\"{}复制到了{}\".format(src_img_path, train_folder))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m                 \u001b[0mtrain_num\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_num\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\ANA\\lib\\shutil.py\u001b[0m in \u001b[0;36mcopy2\u001b[1;34m(src, dst, follow_symlinks)\u001b[0m\n\u001b[0;32m    433\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdst\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    434\u001b[0m         \u001b[0mdst\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdst\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbasename\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 435\u001b[1;33m     \u001b[0mcopyfile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdst\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfollow_symlinks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfollow_symlinks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    436\u001b[0m     \u001b[0mcopystat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdst\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfollow_symlinks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfollow_symlinks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    437\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdst\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\ANA\\lib\\shutil.py\u001b[0m in \u001b[0;36mcopyfile\u001b[1;34m(src, dst, follow_symlinks)\u001b[0m\n\u001b[0;32m    262\u001b[0m         \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msymlink\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadlink\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdst\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    263\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 264\u001b[1;33m         \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfsrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdst\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'wb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfdst\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    265\u001b[0m             \u001b[1;31m# macOS\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    266\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0m_HAS_FCOPYFILE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mPermissionError\u001b[0m: [Errno 13] Permission denied: 'D:/ramanfig\\\\new\\\\test'"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# @Time    : 2021/6/17 20:29\n",
    "# @Author  : dejahu\n",
    "# @Email   : 1148392984@qq.com\n",
    "# @File    : data_split.py\n",
    "# @Software: PyCharm\n",
    "# @Brief   : 将数据集划分为训练集、验证集和测试集\n",
    "import os\n",
    "import random\n",
    "from shutil import copy2\n",
    "\n",
    "\n",
    "def data_set_split(src_data_folder, target_data_folder, train_scale=0.75, val_scale=0, test_scale=0.25):\n",
    "    '''\n",
    "    读取源数据文件夹，生成划分好的文件夹，分为trian、val、test三个文件夹进行\n",
    "    :param src_data_folder: 源文件夹 E:/biye/gogogo/note_book/torch_note/data/utils_test/data_split/src_data\n",
    "    :param target_data_folder: 目标文件夹 E:/biye/gogogo/note_book/torch_note/data/utils_test/data_split/target_data\n",
    "    :param train_scale: 训练集比例\n",
    "    :param val_scale: 验证集比例\n",
    "    :param test_scale: 测试集比例\n",
    "    :return:\n",
    "    '''\n",
    "    print(\"开始数据集划分\")\n",
    "    class_names = os.listdir(src_data_folder)\n",
    "    # 在目标目录下创建文件夹\n",
    "    split_names = ['train', 'val', 'test']\n",
    "    for split_name in split_names:\n",
    "        split_path = os.path.join(target_data_folder, split_name)\n",
    "        if os.path.isdir(split_path):\n",
    "            pass\n",
    "        else:\n",
    "            os.mkdir(split_path)\n",
    "        # 然后在split_path的目录下创建类别文件夹\n",
    "        for class_name in class_names:\n",
    "            class_split_path = os.path.join(split_path, class_name)\n",
    "            if os.path.isdir(class_split_path):\n",
    "                pass\n",
    "            else:\n",
    "                os.mkdir(class_split_path)\n",
    "\n",
    "    # 按照比例划分数据集，并进行数据图片的复制\n",
    "    # 首先进行分类遍历\n",
    "    for class_name in class_names:\n",
    "        current_class_data_path = os.path.join(src_data_folder, class_name)\n",
    "        current_all_data = os.listdir(current_class_data_path)\n",
    "        current_data_length = len(current_all_data)\n",
    "        current_data_index_list = list(range(current_data_length))\n",
    "        random.shuffle(current_data_index_list)\n",
    "\n",
    "        train_folder = os.path.join(os.path.join(target_data_folder, 'train'), class_name)\n",
    "        val_folder = os.path.join(os.path.join(target_data_folder, 'val'), class_name)\n",
    "        test_folder = os.path.join(os.path.join(target_data_folder, 'test'), class_name)\n",
    "        train_stop_flag = current_data_length * train_scale\n",
    "        val_stop_flag = current_data_length * (train_scale + val_scale)\n",
    "        current_idx = 0\n",
    "        train_num = 0\n",
    "        val_num = 0\n",
    "        test_num = 0\n",
    "        for i in current_data_index_list:\n",
    "            src_img_path = os.path.join(current_class_data_path, current_all_data[i])\n",
    "            if current_idx <= train_stop_flag:\n",
    "                copy2(src_img_path, train_folder)\n",
    "                # print(\"{}复制到了{}\".format(src_img_path, train_folder))\n",
    "                train_num = train_num + 1\n",
    "            elif (current_idx > train_stop_flag) and (current_idx <= val_stop_flag):\n",
    "                copy2(src_img_path, val_folder)\n",
    "                # print(\"{}复制到了{}\".format(src_img_path, val_folder))\n",
    "                val_num = val_num + 1\n",
    "            else:\n",
    "                copy2(src_img_path, test_folder)\n",
    "                # print(\"{}复制到了{}\".format(src_img_path, test_folder))\n",
    "                test_num = test_num + 1\n",
    "\n",
    "            current_idx = current_idx + 1\n",
    "\n",
    "        print(\"*********************************{}*************************************\".format(class_name))\n",
    "        print(\n",
    "            \"{}类按照{}：{}：{}的比例划分完成，一共{}张图片\".format(class_name, train_scale, val_scale, test_scale, current_data_length))\n",
    "        print(\"训练集{}：{}张\".format(train_folder, train_num))\n",
    "        print(\"验证集{}：{}张\".format(val_folder, val_num))\n",
    "        print(\"测试集{}：{}张\".format(test_folder, test_num))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    src_data_folder = \"D:/ramanfig\"   # todo 修改你的原始数据集路径\n",
    "    target_data_folder = \"D:/ramanfig/new\"  # todo 修改为你要存放的路径\n",
    "    data_set_split(src_data_folder, target_data_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67500d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# @Time    : 2021/6/17 20:29\n",
    "# @Author  : dejahu\n",
    "# @Email   : 1148392984@qq.com\n",
    "# @File    : train_cnn.py\n",
    "# @Software: PyCharm\n",
    "# @Brief   : cnn模型训练代码，训练的代码会保存在models目录下，折线图会保存在results目录下\n",
    "\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from time import *\n",
    "\n",
    "\n",
    "# 数据集加载函数，指明数据集的位置并统一处理为imgheight*imgwidth的大小，同时设置batch\n",
    "def data_load(data_dir, test_data_dir, img_height, img_width, batch_size):\n",
    "    # 加载训练集\n",
    "    train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "        data_dir,\n",
    "        label_mode='categorical',\n",
    "        seed=123,\n",
    "        image_size=(img_height, img_width),\n",
    "        batch_size=batch_size)\n",
    "    # 加载测试集\n",
    "    val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "        test_data_dir,\n",
    "        label_mode='categorical',\n",
    "        seed=123,\n",
    "        image_size=(img_height, img_width),\n",
    "        batch_size=batch_size)\n",
    "    class_names = train_ds.class_names\n",
    "    # 返回处理之后的训练集、验证集和类名\n",
    "    return train_ds, val_ds, class_names\n",
    "\n",
    "\n",
    "# 构建CNN模型\n",
    "def model_load(IMG_SHAPE=(2048, 2, 1), class_num=50):\n",
    "    # 搭建模型\n",
    "    model = tf.keras.models.Sequential([\n",
    "        # 对模型做归一化的处理，将0-255之间的数字统一处理到0到1之间\n",
    "        tf.keras.layers.experimental.preprocessing.Rescaling(1. / 255, input_shape=IMG_SHAPE),\n",
    "        # 卷积层，该卷积层的输出为32个通道，卷积核的大小是3*3，激活函数为relu\n",
    "        tf.keras.layers.Conv2D(32, (3, 3), activation='relu'),\n",
    "        # 添加池化层，池化的kernel大小是2*2\n",
    "        tf.keras.layers.MaxPooling2D(2, 2),\n",
    "        # Add another convolution\n",
    "        # 卷积层，输出为64个通道，卷积核大小为3*3，激活函数为relu\n",
    "        tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "        # 池化层，最大池化，对2*2的区域进行池化操作\n",
    "        tf.keras.layers.MaxPooling2D(2, 2),\n",
    "        # 将二维的输出转化为一维\n",
    "        tf.keras.layers.Flatten(),\n",
    "        # The same 128 dense layers, and 10 output layers as in the pre-convolution example:\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        # 通过softmax函数将模型输出为类名长度的神经元上，激活函数采用softmax对应概率值\n",
    "        tf.keras.layers.Dense(class_num, activation='softmax')\n",
    "    ])\n",
    "    # 输出模型信息\n",
    "    model.summary()\n",
    "    # 指明模型的训练参数，优化器为sgd优化器，损失函数为交叉熵损失函数\n",
    "    model.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    # 返回模型\n",
    "    return model\n",
    "\n",
    "\n",
    "# 展示训练过程的曲线\n",
    "def show_loss_acc(history):\n",
    "    # 从history中提取模型训练集和验证集准确率信息和误差信息\n",
    "    acc = history.history['accuracy']\n",
    "    val_acc = history.history['val_accuracy']\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "\n",
    "    # 按照上下结构将图画输出\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(acc, label='Training Accuracy')\n",
    "    plt.plot(val_acc, label='Validation Accuracy')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.ylim([min(plt.ylim()), 1])\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.plot(loss, label='Training Loss')\n",
    "    plt.plot(val_loss, label='Validation Loss')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.ylabel('Cross Entropy')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.savefig('results/results_cnn.png', dpi=100)\n",
    "\n",
    "\n",
    "def train(epochs):\n",
    "    # 开始训练，记录开始时间\n",
    "    begin_time = time()\n",
    "    # todo 加载数据集， 修改为你的数据集的路径\n",
    "    train_ds, val_ds, class_names = data_load(\"../data/vegetable_fruit/image_data\",\n",
    "                                              \"../data/vegetable_fruit/test_image_data\", 224, 224, 16)\n",
    "    print(class_names)\n",
    "    # 加载模型\n",
    "    model = model_load(class_num=len(class_names))\n",
    "    # 指明训练的轮数epoch，开始训练\n",
    "    history = model.fit(train_ds, validation_data=val_ds, epochs=epochs)\n",
    "    # todo 保存模型， 修改为你要保存的模型的名称\n",
    "    model.save(\"models/cnn_fv.h5\")\n",
    "    # 记录结束时间\n",
    "    end_time = time()\n",
    "    run_time = end_time - begin_time\n",
    "    print('该循环程序运行时间：', run_time, \"s\")  # 该循环程序运行时间： 1.4201874732\n",
    "    # 绘制模型训练过程图\n",
    "    show_loss_acc(history)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    train(epochs=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db911040",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4b167855",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "4267befc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(850, 2048, 2)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_x=[]\n",
    "path1=os.listdir('D:/dogcat/new_data/train')\n",
    "for i in range(len(path1)):\n",
    "    path2=os.listdir('D:/dogcat/new_data/train/'+path1[i])\n",
    "    for j in range(len(path2)):\n",
    "        data=pd.read_csv('D:/dogcat/new_data/train/'+path1[i]+'/'+path2[j],header=None) \n",
    "        data=data.values\n",
    "        list_x.append(data)\n",
    "x=np.array(list_x)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "899bad41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 2048, 2)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_x=[]\n",
    "path1=os.listdir('D:/dogcat/new_data/val/')\n",
    "for i in range(len(path1)):\n",
    "    path2=os.listdir('D:/dogcat/new_data/val/'+path1[i])\n",
    "    for j in range(len(path2)):\n",
    "        data=pd.read_csv('D:/dogcat/new_data/val/'+path1[i]+'/'+path2[j],header=None) \n",
    "        data=data.values\n",
    "        list_x.append(data)\n",
    "y=np.array(list_x)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5222b9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y=pd.factorize(path1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ad04429e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
       "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4995efb5",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [1000, 50]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-55-64d318be2550>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_test_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.8\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m42\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\Anaconda3\\ANA\\lib\\site-packages\\sklearn\\model_selection\\_split.py\u001b[0m in \u001b[0;36mtrain_test_split\u001b[1;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[0;32m   2170\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"At least one array required as input\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2171\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2172\u001b[1;33m     \u001b[0marrays\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindexable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2173\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2174\u001b[0m     \u001b[0mn_samples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_num_samples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\ANA\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mindexable\u001b[1;34m(*iterables)\u001b[0m\n\u001b[0;32m    297\u001b[0m     \"\"\"\n\u001b[0;32m    298\u001b[0m     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0m_make_indexable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mX\u001b[0m \u001b[1;32min\u001b[0m \u001b[0miterables\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 299\u001b[1;33m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    300\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    301\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\ANA\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    260\u001b[0m     \u001b[0muniques\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    261\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 262\u001b[1;33m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[0m\u001b[0;32m    263\u001b[0m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[0;32m    264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [1000, 50]"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train,x_test,y_train,y_test=train_test_split(x,y,train_size=0.8,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "63a15101",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 2048, 2)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "11a3d446",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "57f8cb85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d_22 (Conv1D)          (None, 2046, 32)          224       \n",
      "                                                                 \n",
      " max_pooling1d_20 (MaxPoolin  (None, 1023, 32)         0         \n",
      " g1D)                                                            \n",
      "                                                                 \n",
      " conv1d_23 (Conv1D)          (None, 1021, 32)          3104      \n",
      "                                                                 \n",
      " max_pooling1d_21 (MaxPoolin  (None, 510, 32)          0         \n",
      " g1D)                                                            \n",
      "                                                                 \n",
      " conv1d_24 (Conv1D)          (None, 508, 64)           6208      \n",
      "                                                                 \n",
      " max_pooling1d_22 (MaxPoolin  (None, 254, 64)          0         \n",
      " g1D)                                                            \n",
      "                                                                 \n",
      " flatten_5 (Flatten)         (None, 16256)             0         \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 64)                1040448   \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 50)                3250      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,053,234\n",
      "Trainable params: 1,053,234\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras import layers\n",
    "from keras import models\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv1D(32, 3, activation='relu',\n",
    "                        input_shape=(2048,2)))\n",
    "model.add(layers.MaxPooling1D(2))\n",
    "model.add(layers.Conv1D(32, 3, activation='relu'))\n",
    "model.add(layers.MaxPooling1D(2))\n",
    "model.add(layers.Conv1D(64,3, activation='relu'))\n",
    "model.add(layers.MaxPooling1D(2))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(50, activation='Softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f2f1dbcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import optimizers\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=optimizers.RMSprop(lr=1e-4),\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4262d374",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Data cardinality is ambiguous:\n  x sizes: 850\n  y sizes: 150\nMake sure all arrays contain the same number of samples.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-42-d7ac0b800cd8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m history = model.fit(\n\u001b[0m\u001b[0;32m      2\u001b[0m       \u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx_val\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m       \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m       batch_size=128)\n",
      "\u001b[1;32mD:\\Anaconda3\\ANA\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m       \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\ANA\\lib\\site-packages\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36m_check_data_cardinality\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m   1655\u001b[0m                            for i in tf.nest.flatten(single_data)))\n\u001b[0;32m   1656\u001b[0m     \u001b[0mmsg\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;34m\"Make sure all arrays contain the same number of samples.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1657\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1658\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1659\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Data cardinality is ambiguous:\n  x sizes: 850\n  y sizes: 150\nMake sure all arrays contain the same number of samples."
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "      x_train,x_val,\n",
    "      epochs=30,\n",
    "      batch_size=128,\n",
    "      validation_data=)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdfa517d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
